{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cassiomatheus/Intelig-ncia-Computacional/blob/main/Adaboost_Trabalho.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Adaboost"
      ],
      "metadata": {
        "id": "CGgaZzHwjNo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questão **1**"
      ],
      "metadata": {
        "id": "gIUIxClj5r2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R = Para a questão indicada, devemos considerar os pesos iniciais normalizados de $ \\mathbf{W_{t}} = [0.1, 0.4, 0.2, 0.1, 0.2]$, onde sua decision stump com menor erro ponderado $\\mathrm{\\epsilon_{t}}$ foi   $\\\\\\mathrm{I} (\\mathrm{x_{1}}\\leqslant1.5)$, calculando os valores $\\mathrm{\\epsilon_{t}}$, $\\mathrm{\\beta_{t}}$, $\\mathrm{\\alpha_{t}}$ e  $\\mathrm{e_{i}}$ (para t=0, tivemos $w_2$ com erro):\n",
        "\n",
        "\n",
        ".$\\tag{1} \\epsilon_t = min_{f,p,\\theta} \\sum_{i=1}^n w_i |h(x_i, f, p, \\theta)-y_i| $\n",
        "\n",
        "\\begin{align}\n",
        "\\epsilon_t = 0,2\\ (w_2=0,2)\n",
        "\\end{align}\n",
        ".$$\\tag{2} \\beta_t = \\frac{\\epsilon_t}{1-\\epsilon_t} =  \\frac{0,2}{1-0,2} = 0,25 $$\n",
        ".\n",
        "$$\\tag{3} \\alpha_t = ln(\\frac{1}{\\beta_t})= 1,38 $$\n",
        ".\n",
        "$$\\tag{4} e_0 =0 , e_1 =0 , e_2 =1,e_3 =0,e_4 =0$$ \n"
      ],
      "metadata": {
        "id": "U9fQEaLx5wZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questão **2**"
      ],
      "metadata": {
        "id": "7SOKOXJqIA38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) O classificador forte possui três stumps determinadas pelo número de interações e seus erros ponderados encontradas em cada stump. ()\n",
        "\n",
        ".$\\tag{1} C(x) = 1 \\ \\mathrm{para} \\sum_{t=1}^T \\alpha_t \\cdot h_t (x) \\geq  \\frac{1}{2} \\sum_{t=1}^T \\alpha_t $\n",
        ".\n",
        "\\begin{align}\n",
        "C(x) = 1.94*0.5+2.56*0.5+2.03*(0.16+0.16+0.16) = 3,27\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "Dxholdz8IFfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) para o classificador forte temos que lembrar a sua taxa de aprendizado (laerning_rate=1), os pesos encontrados para t = 3 e outros parâmetros descritos abaixo:\n",
        "\n",
        ".$\\tag{1} \\epsilon_t = min_{f,p,\\theta} \\sum_{i=1}^n w_i |h(x_i, f, p, \\theta)-y_i| $\n",
        ".\n",
        "\\begin{align}\n",
        "\\epsilon_t = 0,038 +0,038+  0,038=0,11\n",
        "\\end{align}\n",
        ".$$\\tag{2} \\beta_t = \\frac{\\epsilon_t}{1-\\epsilon_t} =  \\frac{0,11}{1-0,11} = 0,13 $$\n",
        ".\n",
        "$$\\tag{3} \\alpha_t = ln(\\frac{1}{\\beta_t})= 2,04 $$\n",
        ".\n",
        "$$\\tag{4} e_0 =0 , e_1 =0 , e_2 =1,e_3 =0,e_4 =0$$ \n"
      ],
      "metadata": {
        "id": "XSG7BCzVN3fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) A stump com maior peso na decisão foi a da terceira interação, onde se obteve maior erro, propondo um peso maior nestas features, considerando que o algoritmo Adaboost necessita dos erros ponderados (obs: atentar-se aos outliers, podendo causar ruídos em seu desempenho)\n",
        "\n",
        "$$W_{3,4} ,W_{3,5}, W_{3,6}.$$ "
      ],
      "metadata": {
        "id": "niJY6wb7T_yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) O valor predito foi 1 para o vetor."
      ],
      "metadata": {
        "id": "1MRN-SFnV2dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E) A escolha foi fornecida através do peso = 8 considaderado na feature $x_0$, o qual temos uma chance de acerto maior com uma zona de decisão próxima a ela, levando em conta $x_0=-2000;y=1$."
      ],
      "metadata": {
        "id": "m-084QdBWB2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "F) Calculando as variáveis t=0:\n",
        "\n",
        "$\\tag{1} \\epsilon_t = min_{f,p,\\theta} \\sum_{i=1}^n w_i |h(x_i, f, p, \\theta)-y_i| $\n",
        ".\n",
        "\\begin{align}\n",
        "\\epsilon_t = w_3 = 0.06\n",
        "\\end{align}\n",
        ".$$\\tag{2} \\beta_t = \\frac{\\epsilon_t}{1-\\epsilon_t} =  \\frac{0,06}{1-0,06} = 0,064 $$\n",
        ".\n",
        "$$\\tag{3} \\alpha_t = ln(\\frac{1}{\\beta_t})= 2,75 $$\n",
        ".\n",
        "$$\\tag{4} e_0 =0 , e_1 =0 , e_2 =0,e_3 =1,e_4 =0,e_5 =0,e_6 =0,e_7 =0$$ "
      ],
      "metadata": {
        "id": "YyI6iIE9YRpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cod."
      ],
      "metadata": {
        "id": "_sWabQAoeBPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.tree import export_text\n",
        "from sklearn.metrics import zero_one_loss\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import numpy as np\n",
        "\n",
        "T = 3 #number of weak learners\n",
        "show_training_info = True\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "def load_lista1_dataset():\n",
        "    #from numpy import genfromtxt\n",
        "    my_data = np.genfromtxt('/content/drive/MyDrive/4 Semestre/Inteligência Computacional/lista1_dataset.csv', delimiter=',')\n",
        "    X = my_data[:,:2] # two first parameters are input vector\n",
        "    #y = my_data[:,2:]\n",
        "    y = np.ravel(my_data[:,2:],order='C') #convert column vector into 1D array\n",
        "    return X,y\n",
        "\n",
        "def load_simple():\n",
        "    #from numpy import genfromtxt\n",
        "    my_data = np.genfromtxt('/content/drive/MyDrive/4 Semestre/Inteligência Computacional/dataset_validation.txt', delimiter=',')\n",
        "    X = my_data[:,:2] # fish length and weight\n",
        "    y = np.ravel(my_data[:,2:],order='C') #convert column vector into 1D array\n",
        "    return X,y\n",
        "\n",
        "def adaboost_predict(x, weak_classifiers, alphas, num_weak_classifiers):\n",
        "    decision_threshold = 0.5 * np.sum(alphas)\n",
        "    score = 0\n",
        "    for t in range(num_weak_classifiers):\n",
        "        score += alphas[t] * weak_classifiers[t].predict(x)\n",
        "    return np.array([score >= decision_threshold]).astype(int)\n",
        "\n",
        "def weighted_error(prediction, correct_label, weights):\n",
        "    error_indices = np.argwhere(prediction != correct_label)\n",
        "    return np.sum(weights[error_indices])\n",
        "\n",
        "if False:\n",
        "    X, y = load_lista1_dataset()\n",
        "else:\n",
        "    X, y = load_simple()\n",
        "print('O valor')\n",
        "print(X)\n",
        "num_training_examples = X.shape[0]\n",
        "\n",
        "#Initialization of instance weights\n",
        "sample_weight = np.array([8, 1, 1, 1, 1, 1, 1, 2]) #zeros(y.shape, dtype=float) \n",
        "index_of_positive_classes = np.argwhere(y==1)\n",
        "index_of_negative_classes = np.argwhere(y==0)\n",
        "num_positive = len(index_of_positive_classes)\n",
        "num_negative = len(index_of_negative_classes)\n",
        "print('Initial sample_weight', sample_weight)\n",
        "\n",
        "# sample_weight[index_of_positive_classes] = 1.0 / (2*num_positive)\n",
        "# sample_weight[index_of_negative_classes] = 1.0 / (2*num_negative)\n",
        "\n",
        "print('Initial sample_weight', sample_weight)\n",
        "\n",
        "weak_classifiers = list()\n",
        "alphas = list()\n",
        "for t in range(T):\n",
        "    print(\"######## Iteration t=\",t)\n",
        "    #1) normalize weight\n",
        "    weights_sum = np.sum(sample_weight)\n",
        "    sample_weight = sample_weight / weights_sum\n",
        "\n",
        "    if show_training_info:\n",
        "        print('sample_weight=',sample_weight)\n",
        "\n",
        "    #2) select the best weak classifier with respect to weighthed error\n",
        "    decision_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\n",
        "    decision_stump.fit(X, y, sample_weight)\n",
        "\n",
        "    #3) Define the weak classifier h_t(x)\n",
        "    weak_classifiers.append(decision_stump)\n",
        "\n",
        "    #4) Update the weights based on the decision of the classifier that\n",
        "    #incorporates all previously designed weak classifiers\n",
        "    y_pred = decision_stump.predict(X)\n",
        "\n",
        "    epsilon_weighted_error = weighted_error(y_pred, y, sample_weight)\n",
        "    #AK: function below return strange, large values\n",
        "    #epsilon_weighted_error = decision_stump.score(X, y, sample_weight)\n",
        "\n",
        "    beta = epsilon_weighted_error / (1.0 - epsilon_weighted_error)\n",
        "    this_alpha = np.log(1.0/beta)\n",
        "    alphas.append(this_alpha)\n",
        "\n",
        "    predictions_of_weak_classifier_t = decision_stump.predict(X)\n",
        "\n",
        "    errors_i = np.array((predictions_of_weak_classifier_t != y)).astype(int)\n",
        "    sample_weight *= beta ** (1.0-errors_i)\n",
        "    #sample_weight = sample_weight / np.sum(sample_weight)\n",
        "\n",
        "    if show_training_info:\n",
        "        print('predictions_of_weak_classifier_t=',predictions_of_weak_classifier_t)\n",
        "        print('                  correct labels=',y)\n",
        "        print('epsilon_weighted_error=', epsilon_weighted_error)\n",
        "        #print('sample_weight=',sample_weight)\n",
        "        print('beta_t=',beta,'alpha_t=',this_alpha)\n",
        "        print('errors_i=',errors_i)\n",
        "\n",
        "for t in range(T):\n",
        "    print('t=', t, ', alpha_t=',alphas[t])\n",
        "    r = export_text(weak_classifiers[t]) #, feature_names=feature_names) #, class_names=target_names)\n",
        "    print(r)\n",
        "print('threshold =', 0.5 * np.sum(alphas))\n",
        "\n",
        "predictions_of_strong_classifier = adaboost_predict(X, weak_classifiers, alphas, T)\n",
        "print('predictions_of_strong_classifier=',predictions_of_strong_classifier)\n",
        "print('                          labels=',y)\n",
        "\n",
        "x = np.genfromtxt('/content/drive/MyDrive/4 Semestre/Inteligência Computacional/simple - Copia.txt', delimiter=',')\n",
        "x1 = x[:,:2] # two first parameters are input vector\n",
        "#y = my_data[:,2:]\n",
        "y1 = np.ravel(x[:,2:],order='C') #convert column vector into 1D array\n",
        "predictions_of_strong_classifier = adaboost_predict(x1, weak_classifiers, alphas, T)\n",
        "print('predictions_of_strong_classifier=',predictions_of_strong_classifier)\n",
        "print('                          labels=',y1)\n",
        "\n",
        "print('\\n\\n#### Comparison with scikit-learn ####')\n",
        "ada_discrete = AdaBoostClassifier(\n",
        "    base_estimator=decision_stump,\n",
        "    learning_rate=1,\n",
        "    n_estimators=T,\n",
        "    algorithm=\"SAMME\")\n",
        "ada_discrete.fit(X, y)\n",
        "ada_discrete_pred = ada_discrete.predict(X)\n",
        "error_indices = np.argwhere(ada_discrete_pred != y)\n",
        "print('error_indices=',error_indices)\n",
        "print('Scikit-learn misclassification error rate (%)=',100.0*len(error_indices)/num_training_examples)\n",
        "\n",
        "print('Scikit-learn AdaBoost classifier:')\n",
        "weak_learners_list = ada_discrete.estimators_\n",
        "for t in range(len(weak_learners_list)):\n",
        "    print('t=',t,'alpha?=',ada_discrete.estimator_weights_[t])\n",
        "    r = export_text(weak_learners_list[t]) #, feature_names=feature_names) #, class_names=target_names)\n",
        "    print(r)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J73GG5IhvqB",
        "outputId": "528cba8a-57f6-4be2-bba2-3a2478cdd06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n",
            "O valor\n",
            "[[ 2.e+03 -2.e+00]\n",
            " [ 4.e+03 -1.e+00]\n",
            " [ 3.e+03 -1.e+00]\n",
            " [-2.e+03  1.e+00]\n",
            " [-6.e+03 -1.e+00]\n",
            " [-4.e+03  0.e+00]\n",
            " [-6.e+03 -4.e+00]\n",
            " [-2.e+03  3.e+00]]\n",
            "Initial sample_weight [8 1 1 1 1 1 1 2]\n",
            "Initial sample_weight [8 1 1 1 1 1 1 2]\n",
            "######## Iteration t= 0\n",
            "sample_weight= [0.5    0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.125 ]\n",
            "predictions_of_weak_classifier_t= [0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "                  correct labels= [0. 0. 0. 0. 1. 1. 1. 1.]\n",
            "epsilon_weighted_error= 0.0625\n",
            "beta_t= 0.06666666666666667 alpha_t= 2.70805020110221\n",
            "errors_i= [0 0 0 1 0 0 0 0]\n",
            "######## Iteration t= 1\n",
            "sample_weight= [0.26666667 0.03333333 0.03333333 0.5        0.03333333 0.03333333\n",
            " 0.03333333 0.06666667]\n",
            "predictions_of_weak_classifier_t= [0. 0. 0. 0. 1. 1. 1. 0.]\n",
            "                  correct labels= [0. 0. 0. 0. 1. 1. 1. 1.]\n",
            "epsilon_weighted_error= 0.06666666666666667\n",
            "beta_t= 0.07142857142857142 alpha_t= 2.6390573296152584\n",
            "errors_i= [0 0 0 0 0 0 0 1]\n",
            "######## Iteration t= 2\n",
            "sample_weight= [0.14285714 0.01785714 0.01785714 0.26785714 0.01785714 0.01785714\n",
            " 0.01785714 0.5       ]\n",
            "predictions_of_weak_classifier_t= [0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "                  correct labels= [0. 0. 0. 0. 1. 1. 1. 1.]\n",
            "epsilon_weighted_error= 0.05357142857142857\n",
            "beta_t= 0.05660377358490566 alpha_t= 2.8716796248840124\n",
            "errors_i= [0 0 0 0 1 1 1 0]\n",
            "t= 0 , alpha_t= 2.70805020110221\n",
            "|--- feature_0 <= 0.00\n",
            "|   |--- class: 1.0\n",
            "|--- feature_0 >  0.00\n",
            "|   |--- class: 0.0\n",
            "\n",
            "t= 1 , alpha_t= 2.6390573296152584\n",
            "|--- feature_0 <= -3000.00\n",
            "|   |--- class: 1.0\n",
            "|--- feature_0 >  -3000.00\n",
            "|   |--- class: 0.0\n",
            "\n",
            "t= 2 , alpha_t= 2.8716796248840124\n",
            "|--- feature_1 <= 2.00\n",
            "|   |--- class: 0.0\n",
            "|--- feature_1 >  2.00\n",
            "|   |--- class: 1.0\n",
            "\n",
            "threshold = 4.10939357780074\n",
            "predictions_of_strong_classifier= [[0 0 0 0 1 1 1 1]]\n",
            "                          labels= [0. 0. 0. 0. 1. 1. 1. 1.]\n",
            "predictions_of_strong_classifier= [[1 0 0 0 0]]\n",
            "                          labels= [0. 0. 1. 1. 1.]\n",
            "\n",
            "\n",
            "#### Comparison with scikit-learn ####\n",
            "error_indices= []\n",
            "Scikit-learn misclassification error rate (%)= 0.0\n",
            "Scikit-learn AdaBoost classifier:\n",
            "t= 0 alpha?= 1.9459101490553132\n",
            "|--- feature_0 <= -3000.00\n",
            "|   |--- class: 1.0\n",
            "|--- feature_0 >  -3000.00\n",
            "|   |--- class: 0.0\n",
            "\n",
            "t= 1 alpha?= 2.5649493574615367\n",
            "|--- feature_0 <= 0.00\n",
            "|   |--- class: 1.0\n",
            "|--- feature_0 >  0.00\n",
            "|   |--- class: 0.0\n",
            "\n",
            "t= 2 alpha?= 2.03688192726104\n",
            "|--- feature_1 <= 2.00\n",
            "|   |--- class: 0.0\n",
            "|--- feature_1 >  2.00\n",
            "|   |--- class: 1.0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}